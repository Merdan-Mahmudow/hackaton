"""–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ Baseline –∏ RuBERT –º–æ–¥–µ–ª–µ–π –ø–æ Macro F1-score."""
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, Optional

import matplotlib.pyplot as plt
import pandas as pd


def load_baseline_metrics(metadata_path: Path) -> Optional[Dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ baseline –º–æ–¥–µ–ª–∏ –∏–∑ metadata.json."""
    if not metadata_path.exists():
        return None
    
    try:
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        metrics = metadata.get('metrics', {})
        macro_f1 = metrics.get('macro avg', {}).get('f1-score', None)
        accuracy = metrics.get('accuracy', None)
        
        if macro_f1 is None:
            return None
        
        return {
            'model_name': 'Baseline (TF-IDF + Logistic Regression)',
            'macro_f1': macro_f1,
            'accuracy': accuracy,
            'metrics': metrics,
            'model_path': metadata.get('model_path', ''),
            'algorithm': metadata.get('algorithm', ''),
            'vectorizer': metadata.get('vectorizer', ''),
        }
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–µ—Ç—Ä–∏–∫ baseline: {e}")
        return None


def load_transformer_metrics(metrics_path: Path) -> Optional[Dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ transformer –º–æ–¥–µ–ª–∏ –∏–∑ transformer_metrics.json."""
    if not metrics_path.exists():
        return None
    
    try:
        with open(metrics_path, 'r', encoding='utf-8') as f:
            metrics = json.load(f)
        
        macro_f1 = metrics.get('eval_macro_f1', None)
        accuracy = metrics.get('eval_accuracy', None)
        
        if macro_f1 is None:
            return None
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –∏–∑ metadata
        metadata_path = Path("models/transformer/metadata.json")
        base_model = "cointegrated/rubert-tiny"
        if metadata_path.exists():
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                base_model = metadata.get('base_model', base_model)
            except Exception:
                pass
        
        return {
            'model_name': f'RuBERT ({base_model.split("/")[-1]})',
            'macro_f1': macro_f1,
            'accuracy': accuracy,
            'metrics': metrics,
            'model_path': str(metadata_path.parent),
            'base_model': base_model,
        }
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–µ—Ç—Ä–∏–∫ transformer: {e}")
        return None


def compare_models(
    baseline_metadata_path: Path = Path("models/metadata.json"),
    transformer_metrics_path: Path = Path("reports/transformer_metrics.json"),
    output_path: Optional[Path] = None,
) -> Dict:
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ Baseline –∏ RuBERT –ø–æ Macro F1-score."""
    
    baseline_metrics = load_baseline_metrics(baseline_metadata_path)
    transformer_metrics = load_transformer_metrics(transformer_metrics_path)
    
    if baseline_metrics is None:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ Baseline –º–æ–¥–µ–ª–∏")
        return {}
    
    if transformer_metrics is None:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ RuBERT –º–æ–¥–µ–ª–∏")
        return {}
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    comparison = {
        'baseline': baseline_metrics,
        'transformer': transformer_metrics,
        'comparison': {
            'macro_f1_diff': transformer_metrics['macro_f1'] - baseline_metrics['macro_f1'],
            'macro_f1_improvement_percent': (
                (transformer_metrics['macro_f1'] - baseline_metrics['macro_f1']) 
                / baseline_metrics['macro_f1'] * 100
            ),
            'accuracy_diff': transformer_metrics['accuracy'] - baseline_metrics['accuracy'],
            'accuracy_improvement_percent': (
                (transformer_metrics['accuracy'] - baseline_metrics['accuracy']) 
                / baseline_metrics['accuracy'] * 100
            ),
        }
    }
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("=" * 80)
    print("–°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô: Baseline vs RuBERT")
    print("=" * 80)
    print(f"\nüìä Baseline (TF-IDF + Logistic Regression):")
    print(f"   Macro F1-score: {baseline_metrics['macro_f1']:.6f}")
    print(f"   Accuracy:       {baseline_metrics['accuracy']:.6f}")
    print(f"   –ê–ª–≥–æ—Ä–∏—Ç–º:       {baseline_metrics.get('algorithm', 'N/A')}")
    print(f"   –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä:   {baseline_metrics.get('vectorizer', 'N/A')}")
    
    print(f"\nü§ñ RuBERT:")
    print(f"   Macro F1-score: {transformer_metrics['macro_f1']:.6f}")
    print(f"   Accuracy:       {transformer_metrics['accuracy']:.6f}")
    print(f"   –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {transformer_metrics.get('base_model', 'N/A')}")
    
    print(f"\nüìà –°–†–ê–í–ù–ï–ù–ò–ï:")
    macro_f1_diff = comparison['comparison']['macro_f1_diff']
    macro_f1_improvement = comparison['comparison']['macro_f1_improvement_percent']
    accuracy_diff = comparison['comparison']['accuracy_diff']
    accuracy_improvement = comparison['comparison']['accuracy_improvement_percent']
    
    print(f"   Macro F1-score:")
    print(f"     –†–∞–∑–Ω–∏—Ü–∞:      {macro_f1_diff:+.6f}")
    print(f"     –£–ª—É—á—à–µ–Ω–∏–µ:    {macro_f1_improvement:+.2f}%")
    
    print(f"   Accuracy:")
    print(f"     –†–∞–∑–Ω–∏—Ü–∞:      {accuracy_diff:+.6f}")
    print(f"     –£–ª—É—á—à–µ–Ω–∏–µ:    {accuracy_improvement:+.2f}%")
    
    if macro_f1_diff > 0:
        print(f"\n‚úÖ RuBERT –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Baseline –Ω–∞ {macro_f1_improvement:.2f}% –ø–æ Macro F1-score")
    else:
        print(f"\n‚ö†Ô∏è Baseline –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    
    print("=" * 80)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if output_path:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(comparison, f, indent=2, ensure_ascii=False)
        print(f"\nüìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")
    
    return comparison


def create_visualization(comparison: Dict, output_path: Optional[Path] = None) -> None:
    """–°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π."""
    if not comparison:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        return
    
    baseline = comparison['baseline']
    transformer = comparison['transformer']
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # –ì—Ä–∞—Ñ–∏–∫ Macro F1-score
    models = [baseline['model_name'], transformer['model_name']]
    macro_f1_scores = [baseline['macro_f1'], transformer['macro_f1']]
    
    bars1 = axes[0].bar(models, macro_f1_scores, color=['#3498db', '#e74c3c'], alpha=0.8)
    axes[0].set_ylabel('Macro F1-score', fontsize=12)
    axes[0].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Macro F1-score', fontsize=14, fontweight='bold')
    axes[0].set_ylim([0, 1.1])
    axes[0].grid(axis='y', alpha=0.3)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for i, (bar, score) in enumerate(zip(bars1, macro_f1_scores)):
        height = bar.get_height()
        axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    f'{score:.4f}',
                    ha='center', va='bottom', fontweight='bold')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ª–∏–Ω–∏—é —É–ª—É—á—à–µ–Ω–∏—è
    improvement = comparison['comparison']['macro_f1_improvement_percent']
    axes[0].annotate(f'–£–ª—É—á—à–µ–Ω–∏–µ: +{improvement:.2f}%',
                    xy=(1, transformer['macro_f1']),
                    xytext=(0.5, transformer['macro_f1'] + 0.15),
                    arrowprops=dict(arrowstyle='->', color='green', lw=2),
                    fontsize=10, fontweight='bold', color='green',
                    ha='center')
    
    # –ì—Ä–∞—Ñ–∏–∫ Accuracy
    accuracy_scores = [baseline['accuracy'], transformer['accuracy']]
    bars2 = axes[1].bar(models, accuracy_scores, color=['#3498db', '#e74c3c'], alpha=0.8)
    axes[1].set_ylabel('Accuracy', fontsize=12)
    axes[1].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Accuracy', fontsize=14, fontweight='bold')
    axes[1].set_ylim([0, 1.1])
    axes[1].grid(axis='y', alpha=0.3)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for i, (bar, score) in enumerate(zip(bars2, accuracy_scores)):
        height = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    f'{score:.4f}',
                    ha='center', va='bottom', fontweight='bold')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ª–∏–Ω–∏—é —É–ª—É—á—à–µ–Ω–∏—è
    acc_improvement = comparison['comparison']['accuracy_improvement_percent']
    axes[1].annotate(f'–£–ª—É—á—à–µ–Ω–∏–µ: +{acc_improvement:.2f}%',
                    xy=(1, transformer['accuracy']),
                    xytext=(0.5, transformer['accuracy'] + 0.15),
                    arrowprops=dict(arrowstyle='->', color='green', lw=2),
                    fontsize=10, fontweight='bold', color='green',
                    ha='center')
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")
    
    plt.show()


def create_detailed_comparison_table(comparison: Dict) -> pd.DataFrame:
    """–°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
    baseline = comparison['baseline']
    transformer = comparison['transformer']
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º –¥–ª—è baseline
    baseline_class_metrics = {}
    if 'metrics' in baseline and isinstance(baseline['metrics'], dict):
        for class_name in ['negative', 'neutral', 'positive']:
            if class_name in baseline['metrics']:
                baseline_class_metrics[class_name] = baseline['metrics'][class_name]
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    data = {
        '–ú–æ–¥–µ–ª—å': ['Baseline', 'RuBERT'],
        'Macro F1-score': [baseline['macro_f1'], transformer['macro_f1']],
        'Accuracy': [baseline['accuracy'], transformer['accuracy']],
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º –¥–ª—è baseline –µ—Å–ª–∏ –µ—Å—Ç—å
    if baseline_class_metrics:
        for class_name in ['negative', 'neutral', 'positive']:
            if class_name in baseline_class_metrics:
                f1 = baseline_class_metrics[class_name].get('f1-score', 0)
                data[f'F1 {class_name}'] = [f1, None]
    
    df = pd.DataFrame(data)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É —Å —Ä–∞–∑–Ω–∏—Ü–µ–π
    diff_row = {
        '–ú–æ–¥–µ–ª—å': '–†–∞–∑–Ω–∏—Ü–∞ (RuBERT - Baseline)',
        'Macro F1-score': comparison['comparison']['macro_f1_diff'],
        'Accuracy': comparison['comparison']['accuracy_diff'],
    }
    
    if baseline_class_metrics:
        for class_name in ['negative', 'neutral', 'positive']:
            if f'F1 {class_name}' in diff_row:
                diff_row[f'F1 {class_name}'] = None
    
    df = pd.concat([df, pd.DataFrame([diff_row])], ignore_index=True)
    
    return df


def main() -> None:
    parser = argparse.ArgumentParser(
        description="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ Baseline –∏ RuBERT –º–æ–¥–µ–ª–µ–π –ø–æ Macro F1-score"
    )
    parser.add_argument(
        "--baseline-metadata",
        type=Path,
        default=Path("models/metadata.json"),
        help="–ü—É—Ç—å –∫ metadata.json baseline –º–æ–¥–µ–ª–∏",
    )
    parser.add_argument(
        "--transformer-metrics",
        type=Path,
        default=Path("reports/transformer_metrics.json"),
        help="–ü—É—Ç—å –∫ transformer_metrics.json",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("reports/model_comparison.json"),
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è",
    )
    parser.add_argument(
        "--plot",
        type=Path,
        default=Path("reports/model_comparison.png"),
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞",
    )
    parser.add_argument(
        "--no-plot",
        action="store_true",
        help="–ù–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫",
    )
    parser.add_argument(
        "--table",
        action="store_true",
        help="–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è",
    )
    
    args = parser.parse_args()
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    comparison = compare_models(
        baseline_metadata_path=args.baseline_metadata,
        transformer_metrics_path=args.transformer_metrics,
        output_path=args.output,
    )
    
    if not comparison:
        return
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    if not args.no_plot:
        try:
            create_visualization(comparison, output_path=args.plot)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {e}")
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    if args.table:
        try:
            df = create_detailed_comparison_table(comparison)
            print("\nüìã –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:")
            print(df.to_string(index=False))
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü—ã: {e}")


if __name__ == "__main__":
    main()

