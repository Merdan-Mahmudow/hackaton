{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML-Web: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\n",
        "\n",
        "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤ Google Colab –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç:\n",
        "- –ó–∞–≥—Ä—É–∂–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "- –û–±—É—á–∞—Ç—å baseline –º–æ–¥–µ–ª—å (TF-IDF + Logistic Regression)\n",
        "- –û–±—É—á–∞—Ç—å transformer –º–æ–¥–µ–ª—å (RuBERT)\n",
        "- –û—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π\n",
        "- –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
        "!pip install -q pandas numpy scikit-learn joblib transformers datasets evaluate torch accelerate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∞—à CSV —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏. –§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏:\n",
        "- `ID` - —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–ø–∏—Å–∏\n",
        "- `text` - —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞\n",
        "- `src` - –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö (–æ—Ç–∫—É–¥–∞ –≤–∑—è—Ç –æ—Ç–∑—ã–≤)\n",
        "- `label` - –º–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞ (0 - neutral, 1 - positive, 2 - negative)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('reports', exist_ok=True)\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª)\n",
        "# uploaded = files.upload()\n",
        "# for filename in uploaded.keys():\n",
        "#     os.rename(filename, f'data/{filename}')\n",
        "\n",
        "# –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–∞–π–ª –∏–∑ Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# # –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª: !cp /content/drive/MyDrive/path/to/train.csv data/train.csv\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ\n",
        "DATA_PATH = 'data/train.csv'  # –ò–∑–º–µ–Ω–∏—Ç–µ –ø—É—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "if os.path.exists(DATA_PATH):\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(df)}\")\n",
        "    print(f\"–ö–æ–ª–æ–Ω–∫–∏: {df.columns.tolist()}\")\n",
        "    \n",
        "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
        "    required_columns = ['ID', 'text', 'src', 'label']\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"\\n‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç\")\n",
        "    \n",
        "    print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (label):\")\n",
        "    print(df['label'].value_counts().sort_index())\n",
        "    \n",
        "    if 'src' in df.columns:\n",
        "        print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (src):\")\n",
        "        print(df['src'].value_counts())\n",
        "        print(f\"\\n–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {df['src'].nunique()}\")\n",
        "    \n",
        "    print(f\"\\n–ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    print(df.head())\n",
        "    \n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "    dataset_info = {\n",
        "        \"total_rows\": len(df),\n",
        "        \"columns\": df.columns.tolist(),\n",
        "        \"label_distribution\": df['label'].value_counts().to_dict() if 'label' in df.columns else {},\n",
        "        \"src_distribution\": df['src'].value_counts().to_dict() if 'src' in df.columns else {},\n",
        "        \"unique_sources\": int(df['src'].nunique()) if 'src' in df.columns else 0,\n",
        "    }\n",
        "    \n",
        "    with open('reports/dataset_info.json', 'w', encoding='utf-8') as f:\n",
        "        import json\n",
        "        json.dump(dataset_info, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"\\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ reports/dataset_info.json\")\n",
        "else:\n",
        "    print(f\"–§–∞–π–ª {DATA_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –≤—ã—à–µ.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "–û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–µ–≥–æ: HTML-—Ç–µ–≥–∏, —Å—Å—ã–ª–∫–∏, –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –∏ —Ç.–¥.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import html\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    \n",
        "    # 1. –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML-—Å—É—â–Ω–æ—Å—Ç–∏\n",
        "    text = html.unescape(text)\n",
        "    \n",
        "    # 2. –£–¥–∞–ª—è–µ–º HTML-—Ç–µ–≥–∏\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    \n",
        "    # 3. –£–¥–∞–ª—è–µ–º URL-–∞–¥—Ä–µ—Å–∞\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    \n",
        "    # 4. –£–¥–∞–ª—è–µ–º email-–∞–¥—Ä–µ—Å–∞\n",
        "    text = re.sub(r'\\S+@\\S+\\.\\S+', ' ', text)\n",
        "    \n",
        "    # 5. –£–¥–∞–ª—è–µ–º –Ω–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ (—Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã)\n",
        "    text = re.sub(r'[\\+]?[78]?[\\s\\-]?\\(?\\d{3}\\)?[\\s\\-]?\\d{3}[\\s\\-]?\\d{2}[\\s\\-]?\\d{2}', ' ', text)\n",
        "    \n",
        "    # 6. –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –±–∞–∑–æ–≤—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
        "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-\\:\\;\\(\\)]+', ' ', text)\n",
        "    \n",
        "    # 7. –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–¥–∏–Ω\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    # 8. –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ\n",
        "    text = text.strip()\n",
        "    \n",
        "    # 9. –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–æ–∂–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å –¥–ª—è transformer-–º–æ–¥–µ–ª–µ–π)\n",
        "    # text = text.lower()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def remove_duplicates(df, subset=['text'], keep='first'):\n",
        "    \"\"\"\n",
        "    –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º.\n",
        "    \"\"\"\n",
        "    original_size = len(df)\n",
        "    df_cleaned = df.drop_duplicates(subset=subset, keep=keep)\n",
        "    removed = original_size - len(df_cleaned)\n",
        "    print(f\"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removed} –∏–∑ {original_size} ({removed/original_size*100:.2f}%)\")\n",
        "    return df_cleaned\n",
        "\n",
        "def remove_short_texts(df, min_length=10, text_column='text'):\n",
        "    \"\"\"\n",
        "    –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤.\n",
        "    \"\"\"\n",
        "    original_size = len(df)\n",
        "    df_cleaned = df[df[text_column].str.len() >= min_length]\n",
        "    removed = original_size - len(df_cleaned)\n",
        "    print(f\"–£–¥–∞–ª–µ–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (< {min_length} —Å–∏–º–≤–æ–ª–æ–≤): {removed} –∏–∑ {original_size} ({removed/original_size*100:.2f}%)\")\n",
        "    return df_cleaned\n",
        "\n",
        "def remove_empty_or_nan(df, text_column='text'):\n",
        "    \"\"\"\n",
        "    –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—É—Å—Ç—ã–º–∏ –∏–ª–∏ NaN —Ç–µ–∫—Å—Ç–∞–º–∏.\n",
        "    \"\"\"\n",
        "    original_size = len(df)\n",
        "    df_cleaned = df.dropna(subset=[text_column])\n",
        "    df_cleaned = df_cleaned[df_cleaned[text_column].str.strip() != '']\n",
        "    removed = original_size - len(df_cleaned)\n",
        "    print(f\"–£–¥–∞–ª–µ–Ω–æ –ø—É—Å—Ç—ã—Ö/NaN —Ç–µ–∫—Å—Ç–æ–≤: {removed} –∏–∑ {original_size} ({removed/original_size*100:.2f}%)\")\n",
        "    return df_cleaned\n",
        "\n",
        "# =============================================================================\n",
        "# –û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–•\n",
        "# =============================================================================\n",
        "\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print(\"=\" * 60)\n",
        "    print(\"–û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–•\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(f\"\\nüìä –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {len(df)} —Å—Ç—Ä–æ–∫\")\n",
        "    \n",
        "    # 1. –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∏ NaN\n",
        "    df = remove_empty_or_nan(df)\n",
        "    \n",
        "    # 2. –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç\n",
        "    print(\"\\nüßπ –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...\")\n",
        "    df['text_original'] = df['text'].copy()  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "    df['text'] = df['text'].apply(clean_text)\n",
        "    \n",
        "    # 3. –£–¥–∞–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã\n",
        "    df = remove_short_texts(df, min_length=10)\n",
        "    \n",
        "    # 4. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
        "    df = remove_duplicates(df, subset=['text'])\n",
        "    \n",
        "    print(f\"\\n‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {len(df)} —Å—Ç—Ä–æ–∫\")\n",
        "    \n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –æ—á–∏—Å—Ç–∫–∏\n",
        "    print(\"\\nüìù –ü—Ä–∏–º–µ—Ä—ã –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞:\")\n",
        "    sample_indices = df.head(3).index\n",
        "    for idx in sample_indices:\n",
        "        orig = df.loc[idx, 'text_original']\n",
        "        clean = df.loc[idx, 'text']\n",
        "        if orig != clean:\n",
        "            print(f\"\\n–û—Ä–∏–≥–∏–Ω–∞–ª: {orig[:100]}...\")\n",
        "            print(f\"–û—á–∏—â–µ–Ω–Ω—ã–π: {clean[:100]}...\")\n",
        "    \n",
        "    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É\n",
        "    df = df.drop(columns=['text_original'])\n",
        "    \n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "    CLEAN_DATA_PATH = 'data/train_clean.csv'\n",
        "    df.to_csv(CLEAN_DATA_PATH, index=False)\n",
        "    print(f\"\\nüíæ –û—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {CLEAN_DATA_PATH}\")\n",
        "    \n",
        "    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "    DATA_PATH = CLEAN_DATA_PATH\n",
        "    print(f\"\\nüîÑ –î–ª—è –æ–±—É—á–µ–Ω–∏—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è: {DATA_PATH}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è –§–∞–π–ª {DATA_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. –û–±—É—á–µ–Ω–∏–µ Baseline –º–æ–¥–µ–ª–∏ (TF-IDF + Logistic Regression)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "texts = df['text'].astype(str)\n",
        "labels = df['label']\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –∏ ID –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
        "if 'src' in df.columns:\n",
        "    srcs = df['src'].astype(str)\n",
        "    print(f\"–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {srcs.value_counts().to_dict()}\")\n",
        "else:\n",
        "    srcs = None\n",
        "\n",
        "if 'ID' in df.columns:\n",
        "    ids = df['ID']\n",
        "    print(f\"–î–∏–∞–ø–∞–∑–æ–Ω ID: {ids.min()} - {ids.max()}\")\n",
        "else:\n",
        "    ids = None\n",
        "\n",
        "# –ú–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫: 0 -> neutral, 1 -> positive, 2 -> negative\n",
        "label_mapping = {0: \"neutral\", 1: \"positive\", 2: \"negative\"}\n",
        "labels = labels.map(label_mapping).fillna(labels.astype(str))\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)\n",
        "train_indices, test_indices = train_test_split(\n",
        "    df.index, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "X_train = texts.iloc[train_indices]\n",
        "X_test = texts.iloc[test_indices]\n",
        "y_train = labels.iloc[train_indices]\n",
        "y_test = labels.iloc[test_indices]\n",
        "\n",
        "# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏\n",
        "if srcs is not None:\n",
        "    train_srcs = srcs.iloc[train_indices]\n",
        "    test_srcs = srcs.iloc[test_indices]\n",
        "    print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ train:\")\n",
        "    print(train_srcs.value_counts())\n",
        "    print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ test:\")\n",
        "    print(test_srcs.value_counts())\n",
        "\n",
        "print(f\"\\nTrain: {len(X_train)}, Test: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# –ù–ê–°–¢–†–û–ô–ö–ò BASELINE –ú–û–î–ï–õ–ò\n",
        "# =============================================================================\n",
        "\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã TF-IDF\n",
        "TFIDF_NGRAM_RANGE = (1, 2)  # –£–Ω–∏–≥—Ä–∞–º–º—ã –∏ –±–∏–≥—Ä–∞–º–º—ã\n",
        "TFIDF_MIN_DF = 1            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞\n",
        "TFIDF_MAX_DF = 0.95         # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞\n",
        "TFIDF_SUBLINEAR_TF = True   # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É–±–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é TF\n",
        "\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã Logistic Regression\n",
        "LR_C = 1.0                  # –û–±—Ä–∞—Ç–Ω–∞—è —Å–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)\n",
        "                            # C ~ 1/learning_rate –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–∞—Ö\n",
        "LR_MAX_ITER = 5000          # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π\n",
        "LR_CLASS_WEIGHT = 'balanced' # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤: 'balanced', None, –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å\n",
        "                            # 'balanced' –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≤–µ—Å–∞ –æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–∞—Å—Ç–æ—Ç–µ –∫–ª–∞—Å—Å–æ–≤\n",
        "LR_SOLVER = 'lbfgs'         # –ê–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: 'lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga'\n",
        "\n",
        "print(\"‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Baseline –º–æ–¥–µ–ª–∏:\")\n",
        "print(f\"  TF-IDF ngram_range: {TFIDF_NGRAM_RANGE}\")\n",
        "print(f\"  TF-IDF min_df: {TFIDF_MIN_DF}\")\n",
        "print(f\"  TF-IDF max_df: {TFIDF_MAX_DF}\")\n",
        "print(f\"  LR C (regularization): {LR_C}\")\n",
        "print(f\"  LR max_iter: {LR_MAX_ITER}\")\n",
        "print(f\"  LR class_weight: {LR_CLASS_WEIGHT}\")\n",
        "print(f\"  LR solver: {LR_SOLVER}\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(\n",
        "        ngram_range=TFIDF_NGRAM_RANGE,\n",
        "        min_df=TFIDF_MIN_DF,\n",
        "        max_df=TFIDF_MAX_DF,\n",
        "        sublinear_tf=TFIDF_SUBLINEAR_TF,\n",
        "    )),\n",
        "    ('clf', LogisticRegression(\n",
        "        C=LR_C,\n",
        "        max_iter=LR_MAX_ITER,\n",
        "        multi_class='auto',\n",
        "        class_weight=LR_CLASS_WEIGHT,\n",
        "        solver=LR_SOLVER,\n",
        "        random_state=42,\n",
        "    )),\n",
        "])\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ\n",
        "print(\"\\nüöÄ –û–±—É—á–µ–Ω–∏–µ baseline –º–æ–¥–µ–ª–∏...\")\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# –ú–µ—Ç—Ä–∏–∫–∏\n",
        "report = classification_report(y_test, y_pred, output_dict=True, digits=4)\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "MODEL_PATH = 'models/baseline.joblib'\n",
        "METADATA_PATH = 'models/metadata.json'\n",
        "\n",
        "joblib.dump(pipeline, MODEL_PATH)\n",
        "\n",
        "# –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n",
        "dataset_metadata = {\n",
        "    \"total_rows\": len(df),\n",
        "    \"train_size\": len(X_train),\n",
        "    \"test_size\": len(X_test),\n",
        "}\n",
        "\n",
        "if srcs is not None:\n",
        "    dataset_metadata[\"sources\"] = {\n",
        "        \"train\": srcs.iloc[train_indices].value_counts().to_dict(),\n",
        "        \"test\": srcs.iloc[test_indices].value_counts().to_dict(),\n",
        "        \"all\": srcs.value_counts().to_dict(),\n",
        "    }\n",
        "    dataset_metadata[\"unique_sources\"] = int(srcs.nunique())\n",
        "\n",
        "metadata = {\n",
        "    \"model_path\": str(MODEL_PATH),\n",
        "    \"algorithm\": \"LogisticRegression\",\n",
        "    \"vectorizer\": \"TfidfVectorizer\",\n",
        "    \"classes\": sorted(labels.unique().tolist()),\n",
        "    \"test_size\": 0.2,\n",
        "    \"random_state\": 42,\n",
        "    \"metrics\": report,\n",
        "    \"dataset\": dataset_metadata,\n",
        "}\n",
        "\n",
        "with open(METADATA_PATH, 'w', encoding='utf-8') as f:\n",
        "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODEL_PATH}\")\n",
        "print(f\"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {METADATA_PATH}\")\n",
        "print(f\"\\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –≤–∫–ª—é—á–µ–Ω–∞ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:\")\n",
        "print(f\"  - –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {dataset_metadata['total_rows']}\")\n",
        "if srcs is not None:\n",
        "    print(f\"  - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {dataset_metadata['unique_sources']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. –û–±—É—á–µ–Ω–∏–µ Transformer –º–æ–¥–µ–ª–∏ (RuBERT)\n",
        "\n",
        "‚ö†Ô∏è **–í–Ω–∏–º–∞–Ω–∏–µ**: –û–±—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ç—Ä–µ–±—É–µ—Ç GPU –≤ Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU\n",
        "import torch\n",
        "print(f\"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"–ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from evaluate import load as load_metric\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# =============================================================================\n",
        "# –ù–ê–°–¢–†–û–ô–ö–ò TRANSFORMER –ú–û–î–ï–õ–ò\n",
        "# =============================================================================\n",
        "\n",
        "# –ú–æ–¥–µ–ª—å\n",
        "MODEL_NAME = \"cointegrated/rubert-tiny\"\n",
        "\n",
        "# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
        "OUTPUT_DIR = \"models/transformer\"\n",
        "\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
        "MAX_LENGTH = 256  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ (256-512 –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤)\n",
        "\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n",
        "EPOCHS = 5                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (3-10)\n",
        "BATCH_SIZE = 16               # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (8-32, –∑–∞–≤–∏—Å–∏—Ç –æ—Ç GPU –ø–∞–º—è—Ç–∏)\n",
        "LEARNING_RATE = 2e-5          # Learning rate (1e-5 - 5e-5 –¥–ª—è fine-tuning BERT)\n",
        "WEIGHT_DECAY = 0.01           # Weight decay –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (0.01 - 0.1)\n",
        "WARMUP_RATIO = 0.1            # –î–æ–ª—è —à–∞–≥–æ–≤ –¥–ª—è warmup (0.0 - 0.2)\n",
        "\n",
        "# Early Stopping –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n",
        "EARLY_STOPPING_ENABLED = True       # –í–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å early stopping\n",
        "EARLY_STOPPING_PATIENCE = 2         # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–æ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n",
        "EARLY_STOPPING_THRESHOLD = 0.0      # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –¥–ª—è —Å—á–∏—Ç–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\n",
        "\n",
        "# –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤\n",
        "USE_CLASS_WEIGHTS = True      # –í–∫–ª—é—á–∏—Ç—å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ò TRANSFORMER –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüì¶ –ú–æ–¥–µ–ª—å: {MODEL_NAME}\")\n",
        "print(f\"\\nüîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:\")\n",
        "print(f\"  –≠–ø–æ—Ö–∏: {EPOCHS}\")\n",
        "print(f\"  –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
        "print(f\"  Warmup ratio: {WARMUP_RATIO}\")\n",
        "print(f\"\\n‚èπÔ∏è Early Stopping:\")\n",
        "print(f\"  –í–∫–ª—é—á–µ–Ω: {EARLY_STOPPING_ENABLED}\")\n",
        "if EARLY_STOPPING_ENABLED:\n",
        "    print(f\"  Patience: {EARLY_STOPPING_PATIENCE} —ç–ø–æ—Ö\")\n",
        "    print(f\"  Threshold: {EARLY_STOPPING_THRESHOLD}\")\n",
        "print(f\"\\n‚öñÔ∏è –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤: {USE_CLASS_WEIGHTS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "texts = df['text'].astype(str)\n",
        "srcs = df['src'].astype(str)\n",
        "labels = df['label']\n",
        "\n",
        "# –ú–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫\n",
        "label_mapping = {0: \"neutral\", 1: \"positive\", 2: \"negative\"}\n",
        "labels = labels.map(label_mapping).fillna(labels.astype(str))\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–ª–∞—Å—Å–æ–≤\n",
        "expected_classes = [\"neutral\", \"positive\", \"negative\"]\n",
        "unique_labels = set(labels.unique().tolist())\n",
        "label2id = {}\n",
        "for idx, cls in enumerate(expected_classes):\n",
        "    if cls in unique_labels:\n",
        "        label2id[cls] = idx\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val\n",
        "train_df, val_df = train_test_split(\n",
        "    pd.DataFrame({\"text\": texts, \"label\": labels.map(label2id), \"src\": srcs}),\n",
        "    test_size=0.2,\n",
        "    stratify=labels.map(label2id),\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
        "print(f\"–ö–ª–∞—Å—Å—ã: {label2id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
        "val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_tokenized = train_tokenized.remove_columns([\"text\"])\n",
        "val_tokenized = val_tokenized.remove_columns([\"text\"])\n",
        "train_tokenized.set_format(\"torch\")\n",
        "val_tokenized.set_format(\"torch\")\n",
        "\n",
        "print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "print(f\"–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {len(label2id)} –∫–ª–∞—Å—Å–æ–≤\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy_metric = load_metric(\"accuracy\")\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    macro_f1 = f1_score(labels, predictions, average=\"macro\")\n",
        "    return {\"accuracy\": accuracy, \"macro_f1\": macro_f1}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# –í–´–ß–ò–°–õ–ï–ù–ò–ï –í–ï–°–û–í –ö–õ–ê–°–°–û–í –î–õ–Ø –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ò\n",
        "# =============================================================================\n",
        "\n",
        "class_weights = None\n",
        "if USE_CLASS_WEIGHTS:\n",
        "    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ train\n",
        "    train_labels = train_df['label'].values\n",
        "    unique_classes = np.unique(train_labels)\n",
        "    class_weights_array = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=unique_classes,\n",
        "        y=train_labels\n",
        "    )\n",
        "    class_weights = torch.tensor(class_weights_array, dtype=torch.float32)\n",
        "    \n",
        "    # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞\n",
        "    if torch.cuda.is_available():\n",
        "        class_weights = class_weights.cuda()\n",
        "    \n",
        "    print(\"‚öñÔ∏è –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:\")\n",
        "    for cls, weight in zip(unique_classes, class_weights_array):\n",
        "        label_name = id2label.get(cls, str(cls))\n",
        "        print(f\"  {label_name} (id={cls}): {weight:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# –ö–ê–°–¢–û–ú–ù–´–ô TRAINER –° WEIGHTED LOSS\n",
        "# =============================================================================\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    \"\"\"Trainer —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π weighted cross-entropy loss –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤.\"\"\"\n",
        "    \n",
        "    def __init__(self, class_weights=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "    \n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        \n",
        "        if self.class_weights is not None:\n",
        "            loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        else:\n",
        "            loss_fn = torch.nn.CrossEntropyLoss()\n",
        "        \n",
        "        loss = loss_fn(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        \n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# =============================================================================\n",
        "# –ù–ê–°–¢–†–û–ô–ö–ê TRAINING ARGUMENTS\n",
        "# =============================================================================\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    eval_strategy=\"epoch\",       # –û—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É\n",
        "    save_strategy=\"epoch\",       # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,   # Warmup –¥–ª—è learning rate\n",
        "    load_best_model_at_end=True, # –ó–∞–≥—Ä—É–∂–∞—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –≤ –∫–æ–Ω—Ü–µ\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    greater_is_better=True,      # –î–ª—è macro_f1 –±–æ–ª—å—à–µ = –ª—É—á—à–µ\n",
        "    logging_steps=50,\n",
        "    save_total_limit=3,          # –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 —á–µ–∫–ø–æ–∏–Ω—Ç–∞\n",
        "    fp16=torch.cuda.is_available(),  # FP16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ GPU\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=\"none\",            # –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ wandb –∏ —Ç.–ø.\n",
        ")\n",
        "\n",
        "print(\"\\nüîß Training Arguments –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã:\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  Weight decay: {training_args.weight_decay}\")\n",
        "print(f\"  Warmup ratio: {training_args.warmup_ratio}\")\n",
        "print(f\"  FP16: {training_args.fp16}\")\n",
        "\n",
        "# =============================================================================\n",
        "# –ù–ê–°–¢–†–û–ô–ö–ê CALLBACKS (EARLY STOPPING)\n",
        "# =============================================================================\n",
        "\n",
        "callbacks = []\n",
        "if EARLY_STOPPING_ENABLED:\n",
        "    early_stopping_callback = EarlyStoppingCallback(\n",
        "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
        "        early_stopping_threshold=EARLY_STOPPING_THRESHOLD,\n",
        "    )\n",
        "    callbacks.append(early_stopping_callback)\n",
        "    print(f\"\\n‚èπÔ∏è Early Stopping –≤–∫–ª—é—á–µ–Ω:\")\n",
        "    print(f\"  Patience: {EARLY_STOPPING_PATIENCE} —ç–ø–æ—Ö\")\n",
        "    print(f\"  Threshold: {EARLY_STOPPING_THRESHOLD}\")\n",
        "\n",
        "# =============================================================================\n",
        "# –°–û–ó–î–ê–ù–ò–ï TRAINER\n",
        "# =============================================================================\n",
        "\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º WeightedTrainer –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤\n",
        "if USE_CLASS_WEIGHTS and class_weights is not None:\n",
        "    trainer = WeightedTrainer(\n",
        "        class_weights=class_weights,\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_tokenized,\n",
        "        eval_dataset=val_tokenized,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "    print(\"\\n‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è WeightedTrainer —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤\")\n",
        "else:\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_tokenized,\n",
        "        eval_dataset=val_tokenized,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "    print(\"\\n‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Trainer\")\n",
        "\n",
        "print(\"\\nüöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û–±—É—á–µ–Ω–∏–µ\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "eval_metrics = trainer.evaluate()\n",
        "print(\"\\n–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:\")\n",
        "for key, value in eval_metrics.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "dataset_metadata = {\n",
        "    \"total_rows\": len(df),\n",
        "    \"train_size\": len(train_dataset),\n",
        "    \"val_size\": len(val_dataset),\n",
        "}\n",
        "\n",
        "if srcs is not None:\n",
        "    dataset_metadata[\"sources\"] = {\n",
        "        \"train\": train_df['src'].value_counts().to_dict() if 'src' in train_df.columns else {},\n",
        "        \"val\": val_df['src'].value_counts().to_dict() if 'src' in val_df.columns else {},\n",
        "        \"all\": srcs.value_counts().to_dict(),\n",
        "    }\n",
        "    dataset_metadata[\"unique_sources\"] = int(srcs.nunique())\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö\n",
        "metadata = {\n",
        "    \"model_type\": \"transformer\",\n",
        "    \"base_model\": MODEL_NAME,\n",
        "    \"model_dir\": OUTPUT_DIR,\n",
        "    \"classes\": list(label2id.keys()),\n",
        "    \"label2id\": label2id,\n",
        "    \"id2label\": {str(idx): label for label, idx in label2id.items()},\n",
        "    \"max_length\": MAX_LENGTH,\n",
        "    \"metrics\": eval_metrics,\n",
        "    \"test_size\": 0.2,\n",
        "    \"random_state\": 42,\n",
        "    \"dataset\": dataset_metadata,\n",
        "    # –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n",
        "    \"training_config\": {\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"warmup_ratio\": WARMUP_RATIO,\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"fp16\": torch.cuda.is_available(),\n",
        "    },\n",
        "    \"early_stopping\": {\n",
        "        \"enabled\": EARLY_STOPPING_ENABLED,\n",
        "        \"patience\": EARLY_STOPPING_PATIENCE if EARLY_STOPPING_ENABLED else None,\n",
        "        \"threshold\": EARLY_STOPPING_THRESHOLD if EARLY_STOPPING_ENABLED else None,\n",
        "    },\n",
        "    \"class_weights\": {\n",
        "        \"enabled\": USE_CLASS_WEIGHTS,\n",
        "        \"weights\": {id2label.get(i, str(i)): float(w) for i, w in enumerate(class_weights_array)} if USE_CLASS_WEIGHTS and class_weights is not None else None,\n",
        "    },\n",
        "}\n",
        "\n",
        "metadata_path = os.path.join(OUTPUT_DIR, \"metadata.json\")\n",
        "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫\n",
        "report_path = \"reports/transformer_metrics.json\"\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(eval_metrics, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {OUTPUT_DIR}\")\n",
        "print(f\"üìÑ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}\")\n",
        "print(f\"üìä –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {report_path}\")\n",
        "print(f\"\\nüìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –≤–∫–ª—é—á–µ–Ω–∞ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:\")\n",
        "print(f\"  - –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {dataset_metadata['total_rows']}\")\n",
        "if srcs is not None:\n",
        "    print(f\"  - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {dataset_metadata['unique_sources']}\")\n",
        "print(f\"\\n‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞:\")\n",
        "print(f\"  - Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  - Early stopping: {'–≤–∫–ª—é—á–µ–Ω' if EARLY_STOPPING_ENABLED else '–≤—ã–∫–ª—é—á–µ–Ω'}\")\n",
        "print(f\"  - Class weights: {'–≤–∫–ª—é—á–µ–Ω—ã' if USE_CLASS_WEIGHTS else '–≤—ã–∫–ª—é—á–µ–Ω—ã'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç—Ä–∏–∫ baseline\n",
        "with open(METADATA_PATH, 'r', encoding='utf-8') as f:\n",
        "    baseline_metrics = json.load(f)\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n",
        "df_full = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–µ–π\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Baseline –º–µ—Ç—Ä–∏–∫–∏\n",
        "baseline_metrics_dict = baseline_metrics.get('metrics', {})\n",
        "if 'accuracy' in baseline_metrics_dict:\n",
        "    baseline_acc = baseline_metrics_dict['accuracy']\n",
        "    baseline_f1 = baseline_metrics_dict.get('macro avg', {}).get('f1-score', 0)\n",
        "else:\n",
        "    baseline_acc = 0\n",
        "    baseline_f1 = 0\n",
        "\n",
        "# Transformer –º–µ—Ç—Ä–∏–∫–∏\n",
        "transformer_acc = eval_metrics.get('eval_accuracy', 0)\n",
        "transformer_f1 = eval_metrics.get('eval_macro_f1', 0)\n",
        "\n",
        "# –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è Accuracy\n",
        "axes[0, 0].bar(['Baseline', 'Transformer'], [baseline_acc, transformer_acc], color=['skyblue', 'lightcoral'])\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Accuracy')\n",
        "axes[0, 0].set_ylim([0, 1])\n",
        "for i, v in enumerate([baseline_acc, transformer_acc]):\n",
        "    axes[0, 0].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è F1\n",
        "axes[0, 1].bar(['Baseline', 'Transformer'], [baseline_f1, transformer_f1], color=['skyblue', 'lightcoral'])\n",
        "axes[0, 1].set_ylabel('Macro F1')\n",
        "axes[0, 1].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Macro F1')\n",
        "axes[0, 1].set_ylim([0, 1])\n",
        "for i, v in enumerate([baseline_f1, transformer_f1]):\n",
        "    axes[0, 1].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
        "if 'src' in df_full.columns:\n",
        "    src_counts = df_full['src'].value_counts()\n",
        "    axes[1, 0].bar(range(len(src_counts)), src_counts.values, color='steelblue')\n",
        "    axes[1, 0].set_xticks(range(len(src_counts)))\n",
        "    axes[1, 0].set_xticklabels(src_counts.index, rotation=45, ha='right')\n",
        "    axes[1, 0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')\n",
        "    axes[1, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (src)')\n",
        "    for i, v in enumerate(src_counts.values):\n",
        "        axes[1, 0].text(i, v + max(src_counts.values) * 0.01, str(v), ha='center', va='bottom')\n",
        "\n",
        "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
        "if 'src' in df_full.columns and 'label' in df_full.columns:\n",
        "    label_mapping = {0: \"neutral\", 1: \"positive\", 2: \"negative\"}\n",
        "    df_full['label_name'] = df_full['label'].map(label_mapping)\n",
        "    pivot_data = pd.crosstab(df_full['src'], df_full['label_name'])\n",
        "    pivot_data.plot(kind='bar', ax=axes[1, 1], color=['#ff9999', '#66b3ff', '#99ff99'])\n",
        "    axes[1, 1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')\n",
        "    axes[1, 1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º')\n",
        "    axes[1, 1].legend(title='–ö–ª–∞—Å—Å', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "else:\n",
        "    axes[1, 1].text(0.5, 0.5, '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö', \n",
        "                     ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "    axes[1, 1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Baseline - Accuracy: {baseline_acc:.4f}, Macro F1: {baseline_f1:.4f}\")\n",
        "print(f\"Transformer - Accuracy: {transformer_acc:.4f}, Macro F1: {transformer_f1:.4f}\")\n",
        "\n",
        "if 'src' in df_full.columns:\n",
        "    print(f\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:\")\n",
        "    print(df_full['src'].value_counts())\n",
        "    if 'label' in df_full.columns:\n",
        "        print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:\")\n",
        "        print(pd.crosstab(df_full['src'], df_full['label_name']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "\n",
        "–°–∫–∞—á–∞–π—Ç–µ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞ —Å –º–æ–¥–µ–ª—è–º–∏\n",
        "import shutil\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –∞—Ä—Ö–∏–≤\n",
        "shutil.make_archive('ml_web_models', 'zip', 'models')\n",
        "shutil.make_archive('ml_web_reports', 'zip', 'reports')\n",
        "\n",
        "print(\"–ê—Ä—Ö–∏–≤—ã —Å–æ–∑–¥–∞–Ω—ã:\")\n",
        "print(\"- ml_web_models.zip (–º–æ–¥–µ–ª–∏)\")\n",
        "print(\"- ml_web_reports.zip (–æ—Ç—á–µ—Ç—ã)\")\n",
        "\n",
        "# –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª—ã\n",
        "files.download('ml_web_models.zip')\n",
        "files.download('ml_web_reports.zip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "\n",
        "–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "from transformers import pipeline\n",
        "\n",
        "# –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "USE_TRANSFORMER = True  # False –¥–ª—è baseline\n",
        "\n",
        "if USE_TRANSFORMER and os.path.exists(OUTPUT_DIR):\n",
        "    # Transformer –º–æ–¥–µ–ª—å\n",
        "    classifier = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=OUTPUT_DIR,\n",
        "        tokenizer=OUTPUT_DIR,\n",
        "        device=0 if torch.cuda.is_available() else -1,\n",
        "    )\n",
        "    print(\"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ transformer –º–æ–¥–µ–ª—å\")\n",
        "else:\n",
        "    # Baseline –º–æ–¥–µ–ª—å\n",
        "    pipeline_model = joblib.load(MODEL_PATH)\n",
        "    print(\"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ baseline –º–æ–¥–µ–ª—å\")\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "test_texts = [\n",
        "    \"–û—Ç–ª–∏—á–Ω—ã–π —Å–µ—Ä–≤–∏—Å, –≤—Å–µ –±—ã—Å—Ç—Ä–æ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ!\",\n",
        "    \"–ü–ª–æ—Ö–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ, –¥–æ–ª–≥–æ –∂–¥–∞–ª –æ—Ç–≤–µ—Ç–∞.\",\n",
        "    \"–û–±—ã—á–Ω—ã–π –¥–µ–Ω—å, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ.\",\n",
        "]\n",
        "\n",
        "print(\"\\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if USE_TRANSFORMER and os.path.exists(OUTPUT_DIR):\n",
        "    for text in test_texts:\n",
        "        result = classifier(text)[0]\n",
        "        print(f\"\\n–¢–µ–∫—Å—Ç: {text}\")\n",
        "        print(f\"–ö–ª–∞—Å—Å: {result['label']}\")\n",
        "        print(f\"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['score']:.4f}\")\n",
        "else:\n",
        "    for text in test_texts:\n",
        "        proba = pipeline_model.predict_proba([text])[0]\n",
        "        classes = pipeline_model.classes_\n",
        "        pred = pipeline_model.predict([text])[0]\n",
        "        print(f\"\\n–¢–µ–∫—Å—Ç: {text}\")\n",
        "        print(f\"–ö–ª–∞—Å—Å: {pred}\")\n",
        "        for cls, score in zip(classes, proba):\n",
        "            print(f\"  {cls}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù –ü–æ–ª–µ–∑–Ω—ã–µ —Å–æ–≤–µ—Ç—ã\n",
        "\n",
        "### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤:\n",
        "- –£–º–µ–Ω—å—à–∏—Ç–µ `EPOCHS` –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "- –£–≤–µ–ª–∏—á—å—Ç–µ `BATCH_SIZE` –µ—Å–ª–∏ –µ—Å—Ç—å GPU —Å –±–æ–ª—å—à–µ–π –ø–∞–º—è—Ç—å—é\n",
        "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `save_total_limit=1` —á—Ç–æ–±—ã —ç–∫–æ–Ω–æ–º–∏—Ç—å –º–µ—Å—Ç–æ\n",
        "\n",
        "### –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏:\n",
        "- `cointegrated/rubert-tiny` - –±—ã—Å—Ç—Ä–∞—è, –ª–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å\n",
        "- `cointegrated/rubert-base` - –±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ\n",
        "- `DeepPavlov/rubert-base-cased-conversational` - –¥–ª—è —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–π —Ä–µ—á–∏\n",
        "\n",
        "### –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Google Drive:\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r models /content/drive/MyDrive/ml_web_models\n",
        "!cp -r reports /content/drive/MyDrive/ml_web_reports\n",
        "```\n",
        "\n",
        "### –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:\n",
        "–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –≤—ã –º–æ–∂–µ—Ç–µ —Å–∫–∞—á–∞—Ç—å –∞—Ä—Ö–∏–≤—ã `ml_web_models.zip` –∏ `ml_web_reports.zip` \n",
        "–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –≤ –≤–∞—à–µ–º –ª–æ–∫–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ ML-Web.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
